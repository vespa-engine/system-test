# Copyright Vespa.ai. All rights reserved.
# Private reason: Depends on pub/ data

require 'performance/nearest_neighbor/common_mips_base'

class AnnMipsWiki < CommonMipsBase

  def setup
    super
    set_owner("geirst")
  end

  def test_mips_with_wiki_dataset
    set_description("Test performance and recall for MIPS using the Wiki simple english dataset")

    # The data set used is Wikipedia simple english from December 2022:
    # https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings
    # This consists of 485851 paragraphs across 187340 wikipedia documents.
    # Each paragraph has a 768-dimensional embedding vector generated by the Cohere multilingual model.
    # The dotproduct distance metric performs best on this model, as described below:
    # https://txt.cohere.com/cross-lingual-classification/
    #
    # The dataset used in this test is created as follows:
    # The 400k first paragraphs are used as documents.
    # The 10k last paragraphs are used as queries.
    #
    # To recreate the data files:
    # 1) Download the four parquet files from https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings
    #    and place them in mips/wiki.
    #
    # 2) Run the script in mips/wiki:
    # python3 create_wiki_data.py docs 400000 > paragraph_docs.400k.json
    # python3 create_wiki_data.py queries 10000 > paragraph_queries.10k.txt
    # python3 create_wiki_data.py vectors 1000 > paragraph_vectors.1k.txt
    #
    # How to upload data files to S3 is described here:
    # https://git.ouryahoo.com/pages/vespa/documentation/documentation/devguide/testing-system-performance-tests.html
 
    @paragraph_docs = @data_path + "paragraph_docs.400k.json"
    @queries = @data_path + "paragraph_queries.10k.txt"
    @query_vectors = @data_path + "paragraph_vectors.1k.txt"

    run_mips_test(selfdir + "mips/wiki/paragraph.sd", @paragraph_docs, "paragraph", "paragraph")
  end

  def teardown
    super
  end

end
