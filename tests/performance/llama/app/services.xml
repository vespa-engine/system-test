<services>

  <admin version="2.0">
    <adminserver hostalias="node1" />
    <configservers>
      <configserver hostalias="node1" />
    </configservers>
  </admin>


  <container version="1.0" id="container">

    <component id="llama" class="ai.vespa.llm.clients.LocalLLM">
      <config name="ai.vespa.llm.clients.llm-local-client">

        <!-- File is approx 130Mb" -->
        <model url="https://data.vespa-cloud.com/gguf_models/Llama-160M-Chat-v1.Q6_K.gguf" />

        <contextSize>8192</contextSize>
        <parallelRequests>40</parallelRequests>
        <maxQueueSize>5</maxQueueSize>
        <maxTokens>100</maxTokens>
        <threads>32</threads>
      </config>
    </component>

    <search>
      <chain id="llm" inherits="vespa">
        <searcher id="ai.vespa.search.llm.LLMSearcher">
          <config name="ai.vespa.search.llm.llm-searcher">
            <providerId>llama</providerId>
          </config>
        </searcher>
      </chain>
    </search>

    <nodes>
      <node hostalias="node1"/>
    </nodes>

  </container>

</services>
